{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlK3mAWOVNuG"
      },
      "source": [
        "# A Guided Introduction to FairlyUncertain\n",
        "\n",
        "In this demo, we will explore some of the functionality of the [FairlyUncertain](https://github.com/rtealwitter/fairlyuncertain) Python package.\n",
        "\n",
        "This notebook is hosted on GitHub. To run it interactively, click the \"Open in Colab\" button below.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rtealwitter/fairlyuncertain/blob/main/examples/demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4G_qKokAw57"
      },
      "outputs": [],
      "source": [
        "# Roughly 1 minute for installation\n",
        "! pip install fairlyuncertain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXaMsp5gAphY"
      },
      "outputs": [],
      "source": [
        "import fairlyuncertain as fu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTXv0ToWGpQx"
      },
      "source": [
        "## Loading Datasets\n",
        "\n",
        "FairlyUncertain is designed in a modular way to enable easy loading of popular fairness datasets. We can check which datasets are available by inspecting the `dataloaders` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbmDm-Z9Govf"
      },
      "outputs": [],
      "source": [
        "# The available datasets\n",
        "list(fu.dataloaders.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5bB9lxV3ri"
      },
      "source": [
        "Let's select the Adult dataset. We can load each dataset by calling the `load_instance` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uka9_b-INim"
      },
      "outputs": [],
      "source": [
        "instance = fu.load_instance('Adult')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W73C6W8gV8yp"
      },
      "source": [
        "An instance is a standardized object used for training. It includes the number of observations `n`, the covariates `X`, the outcomes `y`, and the group indicator `group`. For standardized comparison, it includes a testing-training split (the ratio can be specified as an optional parameter to `load_instance`) on the data. There are also model hyperparameters specified in the instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbJTmlejIW28"
      },
      "outputs": [],
      "source": [
        "print(f\"The {instance['name']} dataset has {instance['n']} observations with {instance['X'].shape[1]} covariates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYsawY42GtGF"
      },
      "source": [
        "## Using Model\n",
        "\n",
        "The foundation of FairlyUncertain is a standardized set of algorithms. Each algorithm accepts the `instance` object and outputs a dictionary with predictions in `pred` and, if relevant to the algorithm, an uncertainty estimate in `std`. We can check which algorithms are available by inspecting the `algorithms` dictionary. (Please see the paper for descriptions of each algorithm.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKpvYWLRHwHA"
      },
      "outputs": [],
      "source": [
        "list(fu.algorithms.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9hLVgRLX2Y6"
      },
      "source": [
        "Let's select the Ensemble algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYeg1RY2H7Xk"
      },
      "outputs": [],
      "source": [
        "algorithm = fu.algorithms['Ensemble']\n",
        "output = algorithm(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD42GWr8X7CU"
      },
      "source": [
        "The output of the algorithm is a set of predictions and heteroscedastic uncertainty estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hilLG5iGIb76"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuY90TsWYADM"
      },
      "source": [
        "Now that we know how to load a dataset and algorithm, let's consider a set of algorithms evaluated on different datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiYh2jvhI5B8"
      },
      "outputs": [],
      "source": [
        "# Several binary algorithms that output uncertainty estimates\n",
        "algo_names = ['Ensemble', 'Selective Ensemble', 'Self-(in)consistency', 'Binomial NLL']\n",
        "# Several binary datasets\n",
        "datasets = ['Adult', 'Bank', 'COMPAS', 'German']\n",
        "is_binary = True # Flag for later evaluation\n",
        "\n",
        "results = {}\n",
        "for dataset in datasets: # Roughly 15 seconds per dataset\n",
        "  instance = fu.load_instance(dataset)\n",
        "  # Instance contains a random testing-training split so we save it for later evaluation\n",
        "  results[dataset] = {'instance' : instance}\n",
        "  for algo_name in algo_names:\n",
        "    algorithm = fu.algorithms[algo_name]\n",
        "    output = algorithm(instance)\n",
        "    results[dataset][algo_name] = output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iFhKqhXGxYk"
      },
      "source": [
        "## Visualizing Calibration\n",
        "\n",
        "We can visualize the calibration of the algorithms by calling `plot_calibration`. For groups of observations with similar uncertainty estimates, the plot shows the average empirical standard deviation against the average predicted uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMEv1ZDkJxwJ"
      },
      "outputs": [],
      "source": [
        "fu.plot_calibration(results, is_binary, algo_names, datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKCw-1EaZ0ED"
      },
      "source": [
        "The dashed gray line indicates the perfectly calibrated model. While not perfect, Binomial NLL is closest to calibrated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvf93CfzR8_x"
      },
      "source": [
        "## Measuring Calibration\n",
        "\n",
        "We can also measure calibration by computing the Negative Log-Likelihood (NLL) for each algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uorf_R9LR_gs"
      },
      "outputs": [],
      "source": [
        "nll_results = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "  instance = results[dataset]['instance']\n",
        "  nll_results[dataset] = {}\n",
        "  for algo_name in algo_names:\n",
        "    output = results[dataset][algo_name]\n",
        "    nll = fu.get_nll(output['pred'], output['std'], instance['y_test'], is_binary)\n",
        "    nll_results[dataset][algo_name] = nll\n",
        "\n",
        "\n",
        "fu.print_table(nll_results, output_latex=False, include_var=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJl2YEvaGgU"
      },
      "source": [
        "As suggested by the plots, Binomial NLL is the most calibrated model as measured by NLL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6ff9AdTGyvC"
      },
      "source": [
        "## Binary Fairness with Abstention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrcXqwBBadiD"
      },
      "source": [
        "We can evaluate the fairness of each algorithm using several standard fairness metrics. In order to incorporate uncertainty, we'll use the abstention framework where models have the option to abstain from predictions with high predicted uncertainty. We'll compare the fairness of models in the abstention framework to standard fairness interventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qaXOaqRM0AF"
      },
      "outputs": [],
      "source": [
        "dataset = 'ACS' # Select a binary dataset\n",
        "\n",
        "instance = fu.load_instance(dataset)\n",
        "\n",
        "algo_names += [\n",
        "    'Baseline', # No fairness intervention\n",
        "    'Exponentiated Gradient EO', # Fairness intervention to minimize equalized odds\n",
        "    'Exponentiated Gradient SP', # Fairness intervention to minimize statistical parity\n",
        "]\n",
        "\n",
        "num_runs = 5\n",
        "results = {}\n",
        "for num_run in range(num_runs): # Roughly 1 minutes per loop\n",
        "  results[num_run] = {'instance' : instance}\n",
        "  for algo_name in algo_names:\n",
        "    algorithm = fu.algorithms[algo_name]\n",
        "    output = algorithm(instance)\n",
        "    results[num_run][algo_name] = output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re2zhsSPjJur"
      },
      "source": [
        "We can print a table to easily inspect the fairness performance below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d2JnEGXdZZm"
      },
      "outputs": [],
      "source": [
        "metric_values = fu.compute_binary_fairness(results, algo_names, fu.binary_metrics)\n",
        "\n",
        "fu.print_table(metric_values, output_latex=False, include_var=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
